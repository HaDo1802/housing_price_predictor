# ğŸ  Housing Price Prediction â€” Production-Grade MLOps Pipeline
<p align="center">
<img src="image/image.png" alt="Real Estate Data Pipeline Cover Image" />
</p>

An end-to-end **production-oriented machine learning pipeline** for predicting housing prices using the Ames Housing dataset.  
This project demonstrates **correct MLOps principles**, including data leakage prevention, train/validation/test separation, experiment tracking, reproducibility, and artifact management.

---

## ğŸ“Œ Overview

This repository implements a **fully modular ML training and inference system**, designed to mirror how models are built, evaluated, and promoted in real-world production environments.

Key goals of this project:
- Predict house prices using structured tabular data
- Apply **proper train / validation / test workflows**
- Track experiments and configurations
- Persist models and preprocessing artifacts for deployment
- Provide a clean separation between training and inference

---

## ğŸ§  Core MLOps Concepts Demonstrated

- âœ… Train / Validation / Test split (no leakage)
- âœ… Preprocessing fitted **only on training data**
- âœ… Validation-based model selection
- âœ… Test set used **once** for final evaluation
- âœ… Configuration-driven pipelines (YAML)
- âœ… Experiment tracking with MLflow
- âœ… Reproducible artifacts (model + preprocessor + metadata)
- âœ… Separate training and inference pipelines

---

## ğŸ“ Project Structure

```
housing_price_predictor/
â”œâ”€â”€ README.md
â”œâ”€â”€ config/                         # Modular config set-up
â”‚   â”œâ”€â”€ config.yaml                 # Entry point for user to customize the config
â”‚   â”œâ”€â”€ default_config.yaml         # Default config to set the baseline
â”‚   â””â”€â”€ config_manager.py           # Modular script that power config-driven setup
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ AmesHousing.csv             # Raw data
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ train.csv
â”‚       â”œâ”€â”€ val.csv
â”‚       â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_split/                 # Modular script handles data split
â”‚   â”‚   â””â”€â”€ data_splitter.py
â”‚   â”‚
â”‚   â”œâ”€â”€ features_engineer/          # Modular script handle feature engineer: scaling, imputer, encoder,..
â”‚   â”‚   â””â”€â”€ preprocessor.py
â”‚   â”‚
â”‚   â””â”€â”€ pipelines/                  # Script that compile others sub-sripts to built full pipelines
â”‚       â”œâ”€â”€ training_pipeline.py    # Modular script handle training pipeline: load, cleaning, split, features engineer, & train
â”‚       â”œâ”€â”€ inference_pipeline.py   # Modular script handle inference pipeline
â”‚       â”œâ”€â”€ fine_tune.py            # Modular script handle fine-tune/ hyperparameter seach
â”‚      
â”‚
â”œâ”€â”€ models/                         # Saved model / artifacts / metadata
â”‚   â””â”€â”€ production/                             
â”‚       â”œâ”€â”€ model.pkl
â”‚       â”œâ”€â”€ preprocessor.pkl
â”‚       â”œâ”€â”€ config.yaml
â”‚       â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ notebook/                       # EDA for understanding data
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â””â”€â”€ Model_Exploration.ipynb     # EDA for understanding model baseline
â”‚
â”œâ”€â”€ docs/                           # Recommed docs for detailed set-up and functionality
â”‚   â”œâ”€â”€ PRODUCTION_STRUCTURE.md
â”‚   â”œâ”€â”€ WORKFLOW_DIAGRAM.md
â”‚   â””â”€â”€ QUICK_REFERENCE.txt
â”‚
â”œâ”€â”€ predict.py
â”œâ”€â”€ requirements.txt                # Python dependencies            
```

---

## ğŸ”„ End-to-End Workflow

```
Raw Data
   â†“
Cleaning (light learning)
   â†“
Train / Val / Test Split   
   â†“
Preprocessing 
   â†“
Model Training
   â†“
Validation Evaluation & Selection
   â†“
Final Training (train + val)
   â†“
Test Evaluation (once)
   â†“
Artifact Saving (model, preprocessor, metadata)
```

---

## âœ¨ Features

### ğŸ§¹ Data Preprocessing

**Data Quality Checks**
- Missing value detection and handling  
- Duplicate/Outlier records removal  
- Data type validation and correction  

**Feature Handling**
- Numerical feature scaling using **StandardScaler**
- Categorical feature encoding handled inbalanced data issue
- Feature schema and ordering persisted for consistent inference

### ğŸ§  Feature Set Used

**Numerical Features**
- Lot Area  
- Total Bsmt SF  
- 1st Flr SF  
- 2nd Flr SF  
- Gr Liv Area  
- Garage Area  
- Overall Qual  
- Overall Cond  
- Year Built  
- Year Remod/Add  
- Bedroom AbvGr  
- Full Bath  
- Half Bath  
- TotRms AbvGrd  
- Fireplaces  
- Garage Cars  

**Categorical Features**
- Neighborhood  
- MS Zoning  
- Bldg Type  
- House Style  
- Foundation  
- Central Air  
- Garage Type  

### âœ‚ï¸ Data Splitting Strategy

- **Training:** 70%  
- **Validation:** 10%  
- **Test:** 20%  
- Split performed **before preprocessing** to prevent data leakage  
- Fixed random seed for reproducibility  

### ğŸ¤– Models Evaluated ( happen in model_exploratory notebook)
- **Linear Regression** â€” baseline model  
- **Ridge / Lasso Regression** â€” regularized linear models  
- **Random Forest Regressor** â€” ensemble of decision trees  
- **Gradient Boosting Regressor** â€” sequential boosting  ==> Best candidate & got chosen for production model!
- **Support Vector Regressor (RBF)** â€” non-linear regression  

### ğŸ“Š Evaluation Metrics

- RÂ²  
- RMSE  
- MAE  
- MSE  

Validation metrics are used for **model selection and tuning**.  
The test set is used **once** for final unbiased evaluation.

### âœ… Best Practices Implemented

- âœ… Train / Validation / Test split to prevent data leakage  
- âœ… Preprocessing fitted **only on training data** to data leakage
- âœ… Validation-based model selection  
- âœ… Test set isolated for final reporting  
- âœ… Configuration-driven pipelines (YAML) ==> enable fully centralized control for users
- âœ… Experiment tracking with MLflow   ==> better view/understanding history runs
- âœ… Model and preprocessor persistence  
- âœ… Reproducible runs via fixed random seeds  
- âœ… Modular code structure  
- âœ… Separate inference pipeline for deployment  

---


## ğŸ§ª Experiment Tracking

This project uses **MLflow** for experiment tracking.

Tracked per run:
- Hyperparameters
- Data split configuration
- Validation and test metrics
- Model artifacts
- Preprocessing configuration

Launch MLflow UI:

```bash
mlflow ui
```

Open: http://localhost:5000

---

## ğŸ³ Docker Containerization

**Why containerize?**
- **Reproducibility:** lock runtime dependencies and OS-level behavior so training/inference behaves the same everywhere.  
- **Portability:** run the same stack locally, on a server, or in CI without environment drift.  
- **Service isolation:** keep MLflow, FastAPI, and Streamlit separated with clear ports and volumes.  
- **Faster onboarding:** one command starts the full stack without manual setup.  

**Set up**

---

## ğŸ“¦ Saved Artifacts

- `model.pkl` â€” trained estimator
- `preprocessor.pkl` â€” fitted preprocessing pipeline
- `config.yaml` â€” training configuration snapshot
- `metadata.json` â€” metrics and feature information

---


## ğŸš€ Why This Project Matters

This project focuses on **engineering discipline**, not just accuracy:
- Prevents data leakage
- Ensures reproducibility
- Mirrors real production ML workflows

---

## ğŸ”® Future Enhancements

- Cross-validation
- Model interpretability (SHAP)
- CI/CD integration
- Drift detection
- API-based inference

---


## ğŸ‘¤ Author

**Ha Do**  
GitHub: https://github.com/HaDo1802
